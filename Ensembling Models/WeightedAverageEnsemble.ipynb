{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE3xqBM_sP_S",
        "outputId": "6f80f612-2c27-4fbf-f004-397880db4439",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git\n",
            "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-o_mdo9so/unsloth_4b1bd42425324ba389dc89062a0256a8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-o_mdo9so/unsloth_4b1bd42425324ba389dc89062a0256a8\n",
            "  Resolved https://github.com/unslothai/unsloth.git to commit 2f2b478868f63b66aaaa93db66ab3d811cddc95e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tyro (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading tyro-0.8.4-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m890.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers>=4.38.2 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.40.2)\n",
            "Collecting datasets>=2.16.0 (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.99)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.43.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.25.2)\n",
            "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.31.0)\n",
            "Collecting xxhash (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.3)\n",
            "Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.11.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.7.1)\n",
            "Collecting shtab>=1.5.6 (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.16.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
            "Building wheels for collected packages: unsloth\n",
            "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unsloth: filename=unsloth-2024.5-py3-none-any.whl size=105132 sha256=62d5849d534a133094e0d01456834231939e841f06453c8063bd28deb116e702\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9tideqfi/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\n",
            "Successfully built unsloth\n",
            "Installing collected packages: xxhash, unsloth, shtab, dill, multiprocess, huggingface-hub, tyro, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 shtab-1.7.1 tyro-0.8.4 unsloth-2024.5 xxhash-3.4.1\n",
            "Collecting xformers<0.0.26\n",
            "  Downloading xformers-0.0.25.post1-cp310-cp310-manylinux2014_x86_64.whl (222.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.5/222.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl\n",
            "  Downloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft\n",
            "  Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes, xformers, trl, peft, accelerate\n",
            "Successfully installed accelerate-0.30.1 bitsandbytes-0.43.1 peft-0.11.1 trl-0.8.6 xformers-0.0.25.post1\n"
          ]
        }
      ],
      "source": [
        "# Requried Installations\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJgWOsMWtDl3",
        "outputId": "54c84676-9828-4db4-8d97-f1f82fae5c76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f909417a650>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQQ3ukzOtFF5",
        "outputId": "82fdf9a4-832b-460f-f842-96f339da3ead"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Weighted Average Ensembling Model Defination</h2>"
      ],
      "metadata": {
        "id": "vYC9-WQjMpDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WtAvgEnsembler(nn.Module):\n",
        "  def __init__(self,model1,model2,tokenizer,modelSize=32000,device=\"cuda\"):\n",
        "    super().__init__()\n",
        "    self.model1 = model1.to(device)\n",
        "    self.model2 = model2.to(device)\n",
        "    self.tokenizer = tokenizer\n",
        "    self.modelSize = modelSize\n",
        "    self.weights = nn.Parameter(torch.tensor([0.5, 0.5])).to(device)\n",
        "    self.linear1 = nn.Linear(self.modelSize,4)\n",
        "    # self.relu = nn.ReLU()\n",
        "    # self.linear2 = nn.Linear(8000,4)\n",
        "\n",
        "  def forward(self,inputIndices,attn_mask):\n",
        "    y1 = self.model1(inputIndices, attn_mask = attn_mask).logits\n",
        "    y2 = self.model2(inputIndices, attn_mask = attn_mask).logits\n",
        "\n",
        "    n,h,w = y1.shape\n",
        "\n",
        "    y1 = y1[:,h-1,:]\n",
        "    y2 = y2[:,h-1,:]\n",
        "\n",
        "\n",
        "    y = y1 * self.weights[0] + y2 * self.weights[1]\n",
        "\n",
        "    y = self.linear1(y)\n",
        "    # y = self.relu(y)\n",
        "    # y = self.linear2(y)\n",
        "\n",
        "    return y"
      ],
      "metadata": {
        "id": "XWDkHpeWtGFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Data Import </h2>"
      ],
      "metadata": {
        "id": "vjEsAQiLMsAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, load_from_disk, concatenate_datasets, Dataset,DatasetDict\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    prepare_model_for_kbit_training,\n",
        "    get_peft_model\n",
        ")\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    MistralForSequenceClassification,\n",
        "    PretrainedConfig,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")"
      ],
      "metadata": {
        "id": "YlCPs3IduqUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdWW5UhYu06Y",
        "outputId": "d7de0175-ca96-4379-cd5c-28b5783cfd3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_location = \"/content/drive/MyDrive/685 Final Project/Datasets/medmcqa-prompts\""
      ],
      "metadata": {
        "id": "BYLVcir6u2Dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset = load_from_disk(f\"{dataset_location}/train_prompts_micro.hf\")\n",
        "# # test_dataset = load_from_disk(f\"{dataset_location}/test_prompts_micro.hf\")\n",
        "# eval_dataset = load_from_disk(f\"{dataset_location}/eval_prompts_micro.hf\")\n",
        "\n",
        "train_dataset = load_from_disk(f\"{dataset_location}/train_prompts_mini.hf\")\n",
        "test_dataset = load_from_disk(f\"{dataset_location}/test_prompts_mini.hf\")\n",
        "eval_dataset = load_from_disk(f\"{dataset_location}/eval_prompts_mini.hf\")"
      ],
      "metadata": {
        "id": "clAYMRBEu3OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISv8oT40u4SD",
        "outputId": "9c04e93c-260a-4d8f-893f-27185f4b93a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name', 'prompt', 'label_one_hot'],\n",
              "    num_rows: 20000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgYZFdMRu7ci",
        "outputId": "a206a456-35c0-4c8a-dd32-b6ecfe3f5efb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name', 'prompt', 'label_one_hot'],\n",
              "    num_rows: 2000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Base Models </h2>"
      ],
      "metadata": {
        "id": "mp48NvMSMwkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained models\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model_location = \"/content/drive/MyDrive/685 Final Project/Models\"\n",
        "\n",
        "model1, tokenizer = FastLanguageModel.from_pretrained(model_location + \"/unsloth_domain1\",\n",
        "                                                     max_seq_length=max_seq_length,\n",
        "                                                     dtype=dtype,\n",
        "                                                     load_in_4bit=load_in_4bit)\n",
        "\n",
        "model2, tokenizer = FastLanguageModel.from_pretrained(model_location + \"/ai2_arc_instruction_tuned_mistral_7b\",\n",
        "                                                     max_seq_length=max_seq_length,\n",
        "                                                     dtype=dtype,\n",
        "                                                     load_in_4bit=load_in_4bit)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmdLlSo8u8k3",
        "outputId": "d022fc8f-b9f3-4df9-8f28-e115281adbe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "==((====))==  Unsloth: Fast Mistral patching release 2024.5\n",
            "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = False.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Fast Mistral patching release 2024.5\n",
            "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = False.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Data Preprocessing</h2>"
      ],
      "metadata": {
        "id": "K7ONsZYoMyrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class MCQDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)  # Changed to float for one-hot encoding\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Function to encode the data\n",
        "def encode_data(tokenizer, prompts):\n",
        "    # encodings = tokenizer(prompts, truncation=True, padding=True, max_length=2048)\n",
        "    encodings = tokenizer(prompts, truncation=True, padding=True)\n",
        "    return encodings\n",
        "\n",
        "# Prepare the data for tokenization\n",
        "prompts = [item['prompt'] for item in train_dataset]\n",
        "labels = [item['label_one_hot'] for item in train_dataset]  # one-hot encoded labels\n",
        "\n",
        "# Tokenize data\n",
        "encodings = encode_data(tokenizer, prompts)\n",
        "\n",
        "# Create dataset\n",
        "train_set = MCQDataset(encodings, labels)\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "\n",
        "\n",
        "prompts = [item['prompt'] for item in eval_dataset]\n",
        "labels = [item['label_one_hot'] for item in eval_dataset]  # one-hot encoded labels\n",
        "\n",
        "# Tokenize data\n",
        "encodings = encode_data(tokenizer, prompts)\n",
        "\n",
        "# Create dataset\n",
        "eval_set = MCQDataset(encodings, labels)\n",
        "\n",
        "# DataLoader\n",
        "val_loader = DataLoader(eval_set, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "JLQJgcobu-Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Model Training and Evaluation </h2>"
      ],
      "metadata": {
        "id": "_mw7jKZEM1Hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from tqdm import tqdm\n",
        "from torch.nn.functional import softmax\n",
        "def train_and_validate(model, train_loader, val_loader, log_file_path,epochs=3):\n",
        "    best_val_acc=0\n",
        "    saved_model_location = \"/content/drive/MyDrive/685 Final Project/Models\"\n",
        "\n",
        "    scaler = GradScaler()\n",
        "    device = torch.device(\"cuda\")\n",
        "    model = model.to(device)  # Ensures model and all submodules are float32\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # for epoch in tqdm(range(epochs)):\n",
        "    with open(log_file_path, 'a') as log_file:\n",
        "      log_file.write(\"Starting training process...\\n\")\n",
        "      log_file.flush()\n",
        "      print(\"Log File created!\")\n",
        "\n",
        "      for epoch in range(epochs):\n",
        "          total_train_loss = 0\n",
        "          total_train_correct = 0\n",
        "          train_samples = 0\n",
        "          model.train()\n",
        "          train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1} [TRAIN]\", unit=\"batch\")\n",
        "          for i, batch in enumerate(train_pbar):\n",
        "              input_ids, labels, attn_mask = batch['input_ids'].to(device), batch['labels'].to(device), batch['attention_mask'].to(device)\n",
        "              train_samples += labels.size(0)\n",
        "              optimizer.zero_grad()\n",
        "              with torch.cuda.amp.autocast():\n",
        "                  output = model(input_ids,attn_mask).float()\n",
        "                  loss = criterion(output, labels.float())\n",
        "                  predictions = torch.argmax(softmax(output,dim=1), dim=1)\n",
        "                  labels_indices = torch.argmax(labels, dim=1)\n",
        "\n",
        "                  train_correct = (predictions == labels_indices).sum().item()\n",
        "                  total_train_correct += train_correct\n",
        "              log_file.write(f\"Batch {i}, Epoch {epoch+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {100 * total_train_correct / train_samples:.2f}%\\n\")\n",
        "              log_file.flush()\n",
        "\n",
        "              scaler.scale(loss).backward()\n",
        "              scaler.step(optimizer)\n",
        "              scaler.update()\n",
        "              total_train_loss += loss.item()\n",
        "\n",
        "              train_pbar.set_postfix(loss=loss.item(), temp_acc=100 * total_train_correct / train_samples)\n",
        "\n",
        "              del input_ids, labels, output, loss, predictions, labels_indices\n",
        "\n",
        "\n",
        "          avg_train_loss = total_train_loss / len(train_loader)\n",
        "          train_accuracy = total_train_correct / train_samples * 100\n",
        "          print(f\"Training Accuracy: \", train_accuracy)\n",
        "          print(f\"Epoch {epoch+1}, Loss: {avg_train_loss}\")\n",
        "\n",
        "          model.eval()\n",
        "          total_val_loss, val_samples, total_val_correct = 0, 0, 0\n",
        "          eval_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1} [EVAL]\", unit=\"batch\")\n",
        "          with torch.no_grad():\n",
        "              for i, batch in enumerate(eval_pbar):\n",
        "                  input_ids, labels, attn_mask = batch['input_ids'].to(device), batch['labels'].to(device), batch['attention_mask'].to(device)\n",
        "                  with torch.cuda.amp.autocast():\n",
        "                      outputs = model(input_ids,attn_mask).float()\n",
        "                      val_loss = criterion(outputs, labels.float())\n",
        "                      predictions = torch.argmax(softmax(outputs,dim=1), dim=1)\n",
        "                      labels_indices = torch.argmax(labels, dim=1)\n",
        "                      total_val_correct += (predictions == labels_indices).sum().item()\n",
        "\n",
        "                  total_val_loss += val_loss.item()\n",
        "                  val_samples += labels.size(0)\n",
        "                  eval_pbar.set_postfix(loss=val_loss.item(), temp_acc=100 * total_val_correct / val_samples)\n",
        "\n",
        "          avg_val_loss = total_val_loss / len(val_loader)\n",
        "          val_accuracy = total_val_correct / val_samples * 100\n",
        "\n",
        "          if val_accuracy > best_val_acc:\n",
        "            best_val_acc=val_accuracy\n",
        "            model_save_path = f\"{saved_model_location}/WtAvgMiniBestEpoch{epoch}.pth\"\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "            print(\"Best model Saved at\", model_save_path)\n",
        "\n",
        "          print(f\"Validation Accuracy: \", val_accuracy)\n",
        "          print(f\"Epoch {epoch+1} - Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Lk27vlcrvJjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "wR87LB-LvPdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lf = WtAvgEnsembler(model1,model2,tokenizer)"
      ],
      "metadata": {
        "id": "Yo4D1iNEvRCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_file_path = '/content/WtAvgEnsemblerMiniLogger.txt'"
      ],
      "metadata": {
        "id": "df1NGavrvSgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_validate(lf,train_loader,val_loader,log_file_path,epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcPwaM6bvYf9",
        "outputId": "5d0cd7a2-cb4a-443f-8bc7-04a5334d0f5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log File created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 [TRAIN]: 100%|██████████| 625/625 [1:31:40<00:00,  8.80s/batch, loss=1.47, temp_acc=24.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy:  24.635\n",
            "Epoch 1, Loss: 1.662692028427124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 [EVAL]: 100%|██████████| 63/63 [01:54<00:00,  1.82s/batch, loss=1.38, temp_acc=19.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model Saved at /content/drive/MyDrive/685 Final Project/Models/WtAvgMiniBestEpoch0.pth\n",
            "Validation Accuracy:  19.650000000000002\n",
            "Epoch 1 - Validation Loss: 1.6659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 [TRAIN]: 100%|██████████| 625/625 [1:31:38<00:00,  8.80s/batch, loss=1.49, temp_acc=25.5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy:  25.515\n",
            "Epoch 2, Loss: 1.5710382879257203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 [EVAL]: 100%|██████████| 63/63 [01:54<00:00,  1.82s/batch, loss=1.53, temp_acc=26.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model Saved at /content/drive/MyDrive/685 Final Project/Models/WtAvgMiniBestEpoch1.pth\n",
            "Validation Accuracy:  26.25\n",
            "Epoch 2 - Validation Loss: 1.6121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 [TRAIN]: 100%|██████████| 625/625 [1:31:38<00:00,  8.80s/batch, loss=1.5, temp_acc=25.9]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy:  25.95\n",
            "Epoch 3, Loss: 1.5173666856765746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 [EVAL]: 100%|██████████| 63/63 [01:54<00:00,  1.82s/batch, loss=1.27, temp_acc=31.9]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model Saved at /content/drive/MyDrive/685 Final Project/Models/WtAvgMiniBestEpoch2.pth\n",
            "Validation Accuracy:  31.85\n",
            "Epoch 3 - Validation Loss: 1.3737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oro_Fd4wwDM9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}