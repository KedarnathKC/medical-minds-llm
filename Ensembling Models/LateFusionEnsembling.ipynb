{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Lk06PMnfGJaY",
        "outputId": "c88cf113-c014-4e5a-ea3c-15d21327cd96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git\n",
            "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-_9lcamof/unsloth_8034fbc622844448bc04ecbe2fe39b7b\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-_9lcamof/unsloth_8034fbc622844448bc04ecbe2fe39b7b\n",
            "  Resolved https://github.com/unslothai/unsloth.git to commit 2f2b478868f63b66aaaa93db66ab3d811cddc95e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tyro (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading tyro-0.8.4-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers>=4.38.2 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.40.2)\n",
            "Collecting datasets>=2.16.0 (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.99)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.43.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.25.2)\n",
            "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.31.0)\n",
            "Collecting xxhash (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.38.2->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.3)\n",
            "Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.11.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.7.1)\n",
            "Collecting shtab>=1.5.6 (from tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.16.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
            "Building wheels for collected packages: unsloth\n",
            "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unsloth: filename=unsloth-2024.5-py3-none-any.whl size=105132 sha256=107137737fba891d750e8cd00df3a071c1d51de1e8f857a879de27b9ddf4cf1d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-tmnlu8wh/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\n",
            "Successfully built unsloth\n",
            "Installing collected packages: xxhash, unsloth, shtab, dill, multiprocess, huggingface-hub, tyro, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 shtab-1.7.1 tyro-0.8.4 unsloth-2024.5 xxhash-3.4.1\n",
            "Collecting xformers<0.0.26\n",
            "  Downloading xformers-0.0.25.post1-cp310-cp310-manylinux2014_x86_64.whl (222.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.5/222.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl\n",
            "  Downloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft\n",
            "  Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes, xformers, trl, peft, accelerate\n",
            "Successfully installed accelerate-0.30.1 bitsandbytes-0.43.1 peft-0.11.1 trl-0.8.6 xformers-0.0.25.post1\n"
          ]
        }
      ],
      "source": [
        "# Requried Installations\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4R82ekHGU_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abc5f09a-834b-408f-b2ac-30e81c739e82"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7a46ef652e30>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLA-xt9kGcCu",
        "outputId": "44f4e19a-7d0b-406d-b590-6768f09f094d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Late Fusion Ensemble Model Defination</h2>"
      ],
      "metadata": {
        "id": "KBddMOOyMLz1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RjDlZZ2GdSc"
      },
      "outputs": [],
      "source": [
        "class LateFusionEnsembler(nn.Module):\n",
        "  def __init__(self,model1,model2,tokenizer,modelSize=32000,device=\"cuda\"):\n",
        "    super().__init__()\n",
        "    self.model1 = model1.to(device)\n",
        "    self.model2 = model2.to(device)\n",
        "    self.tokenizer = tokenizer\n",
        "    self.modelSize = modelSize\n",
        "    self.device = device\n",
        "    self.linear1 = nn.Linear(self.modelSize*2,4)\n",
        "    # self.relu = nn.ReLU()\n",
        "    # self.layernorm = nn.LayerNorm(8000)\n",
        "    # self.linear2 = nn.Linear(8000,4)\n",
        "\n",
        "  def forward(self,inputIndices,attn_mask):\n",
        "    y1 = self.model1(inputIndices,attn_mask=attn_mask).logits\n",
        "    y2 = self.model2(inputIndices,attn_mask=attn_mask).logits\n",
        "\n",
        "    n,h,w = y1.shape\n",
        "\n",
        "    y1 = y1[:,h-1,:]\n",
        "    y2 = y2[:,h-1,:]\n",
        "\n",
        "    y= torch.cat((y1,y2),dim=1)\n",
        "\n",
        "    y = self.linear1(y)\n",
        "    # y = self.relu(y)\n",
        "    # y = self.layernorm(y)\n",
        "    # y = self.linear2(y)\n",
        "\n",
        "    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Data Import </h2>"
      ],
      "metadata": {
        "id": "nHjnHxJIMTT2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yg1Uh7QjI0QW"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_from_disk, concatenate_datasets, Dataset,DatasetDict\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    prepare_model_for_kbit_training,\n",
        "    get_peft_model\n",
        ")\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    MistralForSequenceClassification,\n",
        "    PretrainedConfig,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItIyS29PJAT2",
        "outputId": "b3875c2e-5820-4ffb-a564-3b16066dc153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmoz2IcQJD0P"
      },
      "outputs": [],
      "source": [
        "dataset_location = \"/content/drive/MyDrive/685 Final Project/Datasets/medmcqa-prompts\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vl2knc1tJHOr"
      },
      "outputs": [],
      "source": [
        "# train_dataset = load_from_disk(f\"{dataset_location}/train_prompts_micro.hf\")\n",
        "# # test_dataset = load_from_disk(f\"{dataset_location}/test_prompts_micro.hf\")\n",
        "# eval_dataset = load_from_disk(f\"{dataset_location}/eval_prompts_micro.hf\")\n",
        "\n",
        "train_dataset = load_from_disk(f\"{dataset_location}/train_prompts_mini.hf\")\n",
        "# test_dataset = load_from_disk(f\"{dataset_location}/test_prompts_mini.hf\")\n",
        "eval_dataset = load_from_disk(f\"{dataset_location}/eval_prompts_mini.hf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPJI5SKiJILg",
        "outputId": "d28e445f-b76b-465c-88b3-089f2e435501"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name', 'prompt', 'label_one_hot'],\n",
              "    num_rows: 20000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnAHlnJMJMDQ",
        "outputId": "e46508e0-d167-49dc-bac9-c3b4ccc5caa7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name', 'prompt', 'label_one_hot'],\n",
              "    num_rows: 2000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "eval_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Base Models </h2>"
      ],
      "metadata": {
        "id": "0qKWoPL8MXNs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IO6oNgxoJMyB",
        "outputId": "2fcacb1c-c52e-4f8f-a6da-49275ea4dbe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "==((====))==  Unsloth: Fast Mistral patching release 2024.5\n",
            "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = False.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Fast Mistral patching release 2024.5\n",
            "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = False.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        }
      ],
      "source": [
        "# Load pre-trained models\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model_location = \"/content/drive/MyDrive/685 Final Project/Models\"\n",
        "\n",
        "model1, tokenizer = FastLanguageModel.from_pretrained(model_location + \"/unsloth_domain1\",\n",
        "                                                     max_seq_length=max_seq_length,\n",
        "                                                     dtype=dtype,\n",
        "                                                     load_in_4bit=load_in_4bit)\n",
        "\n",
        "model2, tokenizer = FastLanguageModel.from_pretrained(model_location + \"/ai2_arc_instruction_tuned_mistral_7b_1\",\n",
        "                                                     max_seq_length=max_seq_length,\n",
        "                                                     dtype=dtype,\n",
        "                                                     load_in_4bit=load_in_4bit)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Data Preprocessing</h2>"
      ],
      "metadata": {
        "id": "IBr97DrXMZ5y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qH8phUERJPeV"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class MCQDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)  # Changed to float for one-hot encoding\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Function to encode the data\n",
        "def encode_data(tokenizer, prompts):\n",
        "    # encodings = tokenizer(prompts, truncation=True, padding=True, max_length=2048)\n",
        "    encodings = tokenizer(prompts, truncation=True, padding=True)\n",
        "    return encodings\n",
        "\n",
        "# Prepare the data for tokenization\n",
        "prompts = [item['prompt'] for item in train_dataset]\n",
        "labels = [item['label_one_hot'] for item in train_dataset]  # one-hot encoded labels\n",
        "\n",
        "# Tokenize data\n",
        "encodings = encode_data(tokenizer, prompts)\n",
        "\n",
        "# Create dataset\n",
        "train_set = MCQDataset(encodings, labels)\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "\n",
        "\n",
        "prompts = [item['prompt'] for item in eval_dataset]\n",
        "labels = [item['label_one_hot'] for item in eval_dataset]  # one-hot encoded labels\n",
        "\n",
        "# Tokenize data\n",
        "encodings = encode_data(tokenizer, prompts)\n",
        "\n",
        "# Create dataset\n",
        "eval_set = MCQDataset(encodings, labels)\n",
        "\n",
        "# DataLoader\n",
        "val_loader = DataLoader(eval_set, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Model Training and Evaluation </h2>"
      ],
      "metadata": {
        "id": "xIUcvr7PMctk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtYZ-7DUJk18"
      },
      "outputs": [],
      "source": [
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from tqdm import tqdm\n",
        "from torch.nn.functional import softmax\n",
        "def train_and_validate(model, train_loader, val_loader, log_file_path,epochs=3):\n",
        "    best_val_acc=0\n",
        "    best_avg_val_loss=0\n",
        "    saved_model_location = \"/content/drive/MyDrive/685 Final Project/Models\"\n",
        "\n",
        "    scaler = GradScaler()\n",
        "    device = torch.device(\"cuda\")\n",
        "    model = model.to(device)  # Ensures model and all submodules are float32\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # for epoch in tqdm(range(epochs)):\n",
        "    with open(log_file_path, 'a') as log_file:\n",
        "      log_file.write(\"Starting training process...\\n\")\n",
        "      log_file.flush()\n",
        "      print(\"Log File created!\")\n",
        "\n",
        "      for epoch in range(epochs):\n",
        "          total_train_loss = 0\n",
        "          total_train_correct = 0\n",
        "          train_samples = 0\n",
        "          # correct=list()\n",
        "          model.train()\n",
        "          train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1} [TRAIN]\", unit=\"batch\")\n",
        "          for i, batch in enumerate(train_pbar):\n",
        "              input_ids, labels, attn_mask = batch['input_ids'].to(device), batch['labels'].to(device), batch['attention_mask'].to(device)\n",
        "              train_samples += labels.size(0)\n",
        "              optimizer.zero_grad()\n",
        "              with torch.cuda.amp.autocast():\n",
        "                  output = model(input_ids,attn_mask).float()\n",
        "                  loss = criterion(output, labels.float())\n",
        "                  predictions = torch.argmax(softmax(output,dim=1), dim=1)\n",
        "                  labels_indices = torch.argmax(labels, dim=1)\n",
        "\n",
        "                  train_correct = (predictions == labels_indices).sum().item()\n",
        "                  total_train_correct += train_correct\n",
        "                  # print(\"\\nTotal Correct : \", train_correct)\n",
        "              log_file.write(f\"Batch {i}, Epoch {epoch+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {100 * total_train_correct / train_samples:.2f}%\\n\")\n",
        "              log_file.flush()\n",
        "\n",
        "              scaler.scale(loss).backward()\n",
        "              scaler.step(optimizer)\n",
        "              scaler.update()\n",
        "              total_train_loss += loss.item()\n",
        "\n",
        "              train_pbar.set_postfix(loss=loss.item(), temp_acc=100 * total_train_correct / train_samples)\n",
        "\n",
        "\n",
        "              # if i % 1000 == 0:\n",
        "              #     print(i, loss.item())\n",
        "              #     print(f\"Temp accuracy: \", total_train_correct / train_samples * 100)\n",
        "\n",
        "              # Releasing the memory\n",
        "              del input_ids, labels, output, loss, predictions, labels_indices\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          avg_train_loss = total_train_loss / len(train_loader)\n",
        "          train_accuracy = total_train_correct / train_samples * 100\n",
        "          print(f\"Training Accuracy: \", train_accuracy)\n",
        "          print(f\"Epoch {epoch+1}, Loss: {avg_train_loss}\")\n",
        "\n",
        "          model.eval()\n",
        "          total_val_loss, val_samples, total_val_correct = 0, 0, 0\n",
        "          eval_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1} [EVAL]\", unit=\"batch\")\n",
        "          with torch.no_grad():\n",
        "              for i, batch in enumerate(eval_pbar):\n",
        "                  input_ids, labels, attn_mask = batch['input_ids'].to(device), batch['labels'].to(device), batch['attention_mask'].to(device)\n",
        "                  with torch.cuda.amp.autocast():\n",
        "                      outputs = model(input_ids,attn_mask).float()\n",
        "                      val_loss = criterion(outputs, labels.float())\n",
        "                      predictions = torch.argmax(softmax(outputs,dim=1), dim=1)\n",
        "                      labels_indices = torch.argmax(labels, dim=1)\n",
        "                      total_val_correct += (predictions == labels_indices).sum().item()\n",
        "\n",
        "                  total_val_loss += val_loss.item()\n",
        "                  val_samples += labels.size(0)\n",
        "                  eval_pbar.set_postfix(loss=val_loss.item(), temp_acc=100 * total_val_correct / val_samples)\n",
        "\n",
        "          avg_val_loss = total_val_loss / len(val_loader)\n",
        "          val_accuracy = total_val_correct / val_samples * 100\n",
        "\n",
        "\n",
        "          if (val_accuracy > best_val_acc or (val_accuracy == best_val_acc and avg_val_loss <= best_avg_val_loss)):\n",
        "            best_val_acc=val_accuracy\n",
        "            best_avg_val_loss = avg_val_loss\n",
        "            model_save_path = f\"{saved_model_location}/LateFusionMiniBest1Epoch{epoch}.pth\"\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "            print(\"Best model Saved at\", model_save_path)\n",
        "\n",
        "\n",
        "          print(f\"Validation Accuracy: \", val_accuracy)\n",
        "          print(f\"Epoch {epoch+1} - Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fq0AO-mrKTW1"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJUzSYNRKVqu"
      },
      "outputs": [],
      "source": [
        "lf = LateFusionEnsembler(model1,model2,tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2wrT5FDMYH8"
      },
      "outputs": [],
      "source": [
        "log_file_path = '/content/LateFusionMiniLogger2.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BVHG2erKg7d",
        "outputId": "9d24b4c3-79a5-4b03-f5e4-1be69ab33871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log File created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 [TRAIN]: 100%|██████████| 625/625 [1:31:35<00:00,  8.79s/batch, loss=1.45, temp_acc=25.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy:  25.275\n",
            "Epoch 1, Loss: 2.02354776058197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 [EVAL]: 100%|██████████| 63/63 [01:54<00:00,  1.82s/batch, loss=1.95, temp_acc=22.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model Saved at /content/drive/MyDrive/685 Final Project/Models/LateFusionMiniBest1Epoch0.pth\n",
            "Validation Accuracy:  22.25\n",
            "Epoch 1 - Validation Loss: 1.9989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 [TRAIN]: 100%|██████████| 625/625 [1:31:35<00:00,  8.79s/batch, loss=1.8, temp_acc=26]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy:  26.029999999999998\n",
            "Epoch 2, Loss: 1.7389952281951904\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 [EVAL]: 100%|██████████| 63/63 [01:54<00:00,  1.82s/batch, loss=1.04, temp_acc=31.9]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model Saved at /content/drive/MyDrive/685 Final Project/Models/LateFusionMiniBest1Epoch1.pth\n",
            "Validation Accuracy:  31.85\n",
            "Epoch 2 - Validation Loss: 1.6305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 [TRAIN]:  47%|████▋     | 294/625 [43:04<48:30,  8.79s/batch, loss=1.48, temp_acc=25.4]"
          ]
        }
      ],
      "source": [
        "train_and_validate(lf,train_loader,val_loader,log_file_path,epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UlAux2VQWxV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRS4hBpBboc6"
      },
      "outputs": [],
      "source": [
        "# from google.colab import runtime\n",
        "# runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6fykkm0qj4S"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}