# -*- coding: utf-8 -*-
"""Noisy_TopK_Mixture_Of_Experts.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iDLv0PgPpLI9sVx1aybcCTLKLSexWbtp
"""

from google.colab import drive
drive.mount('/content/drive')

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(device)

# !pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
# !pip install --no-deps "xformers<0.0.26" trl peft accelerate bitsandbytes
# !pip install accelerate

# Load finetuned models
from unsloth import FastLanguageModel

max_seq_length = 256 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

model1, tokenizer = FastLanguageModel.from_pretrained("/content/drive/MyDrive/685 Final Project/Models/unsloth_domain1",
                                                     max_seq_length=max_seq_length,
                                                     dtype=dtype,
                                                     load_in_4bit=load_in_4bit)

model2, tokenizer = FastLanguageModel.from_pretrained("/content/drive/MyDrive/685 Final Project/Models/ai2_arc_instruction_tuned_mistral_7b",
                                                     max_seq_length=max_seq_length,
                                                     dtype=dtype,
                                                     load_in_4bit=load_in_4bit)

# for param in model1.parameters():
#     param.requires_grad = False

# for param in model2.parameters():
#     param.requires_grad = False

models = [model1, model2]

#routing layer
class NoisyTopkRouter(nn.Module):
    def __init__(self, n_embed, num_experts, top_k=1):
        super(NoisyTopkRouter, self).__init__()
        self.top_k = top_k
        #layer for router logits
        self.topkroute_linear = nn.Linear(n_embed, num_experts).to(device)
        self.noise_linear =nn.Linear(n_embed, num_experts).to(device)

    def forward(self, z):
        #z = z.half()
        logits = self.topkroute_linear(z)

        #Noise logits
        noise_logits = self.noise_linear(z)

        #Adding scaled unit gaussian noise to the logits
        noise = torch.randn_like(logits)*F.softplus(noise_logits)
        noisy_logits = logits + noise

        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)
        zeros = torch.full_like(noisy_logits, float('-inf'))
        sparse_logits = zeros.scatter(-1, indices, top_k_logits)
        router_output = F.softmax(sparse_logits, dim=-1)
        return router_output, indices

T = 300
class MoEModel(nn.Module):
    def __init__(self, experts, input_dim, top_k, capacity_factor=1.0):
        super(MoEModel, self).__init__()

        self.top_k = top_k
        self.capacity_factor = capacity_factor

        #fine-tuned experts
        self.experts = nn.ModuleList(experts)

        #number of layers to iterate MOE
        self.num_layers = len(experts[0].base_model.model.model.layers)  # Correct path to access layers

        #top k noisy routing mechanism
        self.gating =  nn.ModuleList([NoisyTopkRouter(input_dim, len(experts)) for _ in range(self.num_layers)])

        #Learned Positional Embedding
        self.position_embedding_table = nn.Embedding(T, input_dim).to(device)

        #Layer Normalization for mitigating vanishing gradient, z to smoe
        self.ln1 = nn.ModuleList([nn.LayerNorm(input_dim, eps=1e-4) for _ in range(self.num_layers)])
        #Layer Normalization for mitigating vanishing gradient, final layer
        self.ln_f = nn.LayerNorm(input_dim,eps=1e-4).to(device)

        #Final linear layer logits
        self.output_layer = nn.Linear(input_dim, 4).to(device)

        #Proababilities from logits
        #self.softmax = nn.LogSoftmax(dim=1).to(device)

    def forward(self, input_ids):
        #Batch, Tokens
        B,T = input_ids.shape
        #Embed input_ids to match expert models embedding space
        token_embeddings = self.experts[0].base_model.model.model.embed_tokens(input_ids).to(torch.float32) #[B,T,C]
        #Positional Embeddings for the tokens
        positional_embeddings = self.position_embedding_table(torch.arange(T, device=device)) # [T,C]
        x = torch.add(token_embeddings, positional_embeddings) #[B,T,C]
        final_output = torch.zeros_like(x) #[B,T,C]
        for i in range(self.num_layers):

            B,T,C = x.shape
            # print("Shape of x", x.shape)
            #x = x.half()
            # print("x in forward:",x)
            x_sc = x
            x = self.ln1[i](x)
            # with open("output.txt", "a") as file:
            #   file.write("/nLayer {}: x_sc{}: x_ln1{}: ".format(i, x_sc, x))
            # print("x_sc in forward:",x_sc)
            # print("x_ln1 in forward:",x)
            # get the expert to route to
            gating_output, expert_indices = self.gating[i](x)
            # print("gating_output in forward:",gating_output.shape)

            layer_output = torch.zeros_like(x)
            # Flatten the batch and sequence dimensions to treat each token independently
            flat_x = x.view(-1, x.size(-1))
            # print("flat_x in forward:",flat_x.shape)
            flat_gating_output = gating_output.view(-1, gating_output.size(-1))
            # print("flat_gating_output in forward:",flat_gating_output.shape)
            #tokens per expert according to capacity factor
            tokens_per_batch = B * T * self.top_k
            expert_capacity = int((tokens_per_batch / len(self.experts)) * self.capacity_factor)

            #updates
            updates = torch.zeros_like(flat_x)

            # print('x', x.shape)
            for idx, expert in enumerate(self.experts):
                expert_mask = (expert_indices == idx).any(dim=-1)
                # print('expert mask', expert_mask)
                flat_mask = expert_mask.view(-1)

                selected_indices = torch.nonzero(flat_mask).squeeze(-1)
                limited_indices = selected_indices[:expert_capacity] if selected_indices.numel() > expert_capacity else selected_indices

                if limited_indices.numel() > 0:
                  expert_input = flat_x[limited_indices]
                  # print('expert_input size',expert_input.shape)
                  T_in,C_in = expert_input.shape
                  expert_input_mistral = expert_input.view(1,-1,C_in)
                  # print('expert_input mistral shape',expert_input_mistral.shape)
                  #expert_mistral_input.half()
                  expert_output = expert.base_model.model.model.layers[i](expert_input_mistral)[0]
                  # print('expert output shape',expert_output.shape)
                  flat_expert_output = expert_output.view(-1, expert_output.size(-1))
                  # print('flat expert output shape',flat_expert_output.shape)
                  gating_scores = flat_gating_output[limited_indices, idx].unsqueeze(1)
                  # print('gating scores shape', gating_scores.shape)
                  weighted_output = torch.multiply(flat_expert_output , gating_scores)
                  # print('weighted output shape', weighted_output.shape)
                  # print('limited_indices shape', limited_indices.shape)
                  updates.index_add_(0, limited_indices, weighted_output)
                  # with open("output.txt", "a") as file:
                  #   file.write("/nLayer {}: idx {}: limited_indices{}: expert_input{}: expert_input_mistral{}: expert_output{}: flat_expert_output{}: gating_scores{}: weighted_output{}: updates{}: ".format(i, idx, limited_indices, expert_input, expert_input_mistral, expert_output, flat_expert_output, gating_scores, weighted_output,updates))

            # Reshape updates to match the original dimensions of x
            final_output += updates.view(B, T, -1)
            # print("Final output shape", final_output.shape)
            x = torch.add(x_sc, final_output)


        # print('x1',x.shape)
        #x = x.transpose(1, 2)  # Adjust dimensions for pooling
        #x = self.pooling(x).squeeze(2)
        # print('x2', x.shape)
        x = self.ln_f(x)
        x = self.output_layer(x)
        # print("x_final in forward before softmax:", x.shape)
        # print("x_final in forward before softmax:", x)
        x = x.mean(dim=1)
        # print("x_final mean in forward:", x.shape)
        # print("x_final mean in forward:", x)
        #x = self.softmax(x)
        # print("x_final mean softmax in forward:", x.shape)
        # print("x_final mean softmax in forward:", x)


        return x

moe_model = MoEModel(models, input_dim=4096, top_k=1)
tokenizer.pad_token = tokenizer.eos_token

#Glorot Weight Initialization
# from torch.nn.init import kaiming_uniform_
# def glorot_init_weights(m):
#     if isinstance (m, (nn.Linear)):
#       kaiming_uniform_(m.weight)

#moe_model.apply(glorot_init_weights)

from datasets import load_dataset, load_from_disk, concatenate_datasets, Dataset

dataset_location = '/content/drive/MyDrive/685 Final Project/Datasets/medmcqa-prompts'

train_dataset = load_from_disk(f"{dataset_location}/train_prompts_mini.hf")
# test_dataset = load_from_disk(f"{dataset_location}/test_prompts.hf")
eval_dataset = load_from_disk(f"{dataset_location}/eval_prompts_mini.hf")

# print(train_dataset)

from torch.utils.data import DataLoader, Dataset

class MCQDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float16)  # Changed to float for one-hot encoding
        return item

    def __len__(self):
        return len(self.labels)

# Function to encode the data
def encode_data(tokenizer, prompts):
    encodings = tokenizer(prompts, truncation=True, padding=True, max_length=300)
    return encodings

# Prepare the data for tokenization
prompts = [item['prompt'] for item in train_dataset]
labels = [item['label_one_hot'] for item in train_dataset]  # one-hot encoded labels

# Tokenize data
encodings = encode_data(tokenizer, prompts)

# Create dataset
train_set = MCQDataset(encodings, labels)

# DataLoader
train_loader = DataLoader(train_set, batch_size=8, shuffle=True)

prompts = [item['prompt'] for item in eval_dataset]
labels = [item['label_one_hot'] for item in eval_dataset]  # one-hot encoded labels

# Tokenize data
encodings = encode_data(tokenizer, prompts)

# Create dataset
eval_set = MCQDataset(encodings, labels)

# DataLoader
val_loader = DataLoader(eval_set, batch_size=8, shuffle=True)

import torch
import torch.nn.functional as F
from torch.cuda.amp import GradScaler, autocast
from tqdm import tqdm

def train_and_validate(model, train_loader, val_loader, epochs=8, log_file_path = "/content/drive/MyDrive/685 Final Project/Models/TopKMoE/training_logs.txt"):

    saved_model_location = "/content/drive/MyDrive/685 Final Project/Models/TopKMoE"
    scaler = GradScaler()
    device = torch.device("cuda")
    model = model.to(device)  # Ensures model and all submodules are float32
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, eps=1e-4, weight_decay=1e-3)
    criterion = torch.nn.CrossEntropyLoss()
    best_val_accuracy = 0.0

    with open(log_file_path, 'a') as log_file:
        log_file.write("Starting training process...\n")
        log_file.flush()
        for epoch in range(epochs):
          total_train_loss = 0
          total_train_correct = 0
          train_samples = 0

          model.train()
          train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1} [TRAIN]", unit="batch")
          for i, batch in enumerate(train_pbar):
              input_ids, labels = batch['input_ids'].to(device), batch['labels'].to(device)
              train_samples += labels.size(0)
              optimizer.zero_grad()
              with torch.cuda.amp.autocast():
                  output = model(input_ids).float()
                  loss = criterion(output, labels.float())
                  predictions = torch.argmax(F.softmax(output,dim=1), dim=1)
                  labels_indices = torch.argmax(labels, dim=1)
                  # print(output)
                  # print(predictions)
                  # print(labels)
                  total_train_correct += (predictions == labels_indices).sum().item()

              # loss = loss.to(torch.float32)
              scaler.scale(loss).backward()
              scaler.step(optimizer)
              scaler.update()
              total_train_loss += loss.item()

              log_file.write(f"Batch {i}, Epoch {epoch+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {100 * total_train_correct / train_samples:.2f}%\n")
              log_file.flush()

              train_pbar.set_postfix(loss=loss.item(), temp_acc=100 * total_train_correct / train_samples)

              if i % 100 == 0:
                  print(i, loss.item())
                  print(f"Temp accuracy: ", total_train_correct / train_samples * 100)
              del input_ids, labels, output, loss, predictions, labels_indices

          model_save_path = f"{saved_model_location}/TopKMoE"+f"{epoch}.pth"
          torch.save(model.state_dict(), model_save_path)
          print("model Saved at", model_save_path)

          avg_train_loss = total_train_loss / len(train_loader)
          train_accuracy = total_train_correct / train_samples * 100
          print(f"Training Accuracy: ", train_accuracy)
          print(f"Epoch {epoch+1}, Loss: {avg_train_loss}")

          model.eval()
          total_val_loss, val_samples, total_val_correct = 0, 0, 0
          val_pbar = tqdm(val_loader, desc=f"Epoch {epoch+1} [VAL]", unit="batch")
          # with torch.no_grad():
          for i, batch in enumerate(val_pbar):
              input_ids, labels = batch['input_ids'].to(device), batch['labels'].to(device)
              with torch.cuda.amp.autocast():
                  output = model(input_ids).float()
                  val_loss = criterion(output, labels.float())
                  predictions = torch.argmax(F.softmax(output,dim=1), dim=1)
                  labels_indices = torch.argmax(labels, dim=1)
                  total_val_correct += (predictions == labels_indices).sum().item()

              total_val_loss += val_loss.item()
              val_samples += labels.size(0)
              log_file.write(f"Batch {i}, Epoch {epoch+1}, Validation Loss: {val_loss.item():.4f}, Validation Accuracy: {100 * total_val_correct / val_samples:.2f}%\n")
              log_file.flush()
              del input_ids, labels, output, val_loss, predictions, labels_indices

          avg_val_loss = total_val_loss / len(val_loader)
          val_accuracy = total_val_correct / val_samples * 100
          print(f"Validation Accuracy: ", val_accuracy)
          print(f"Epoch {epoch+1} - Validation Loss: {avg_val_loss:.4f}")

          if val_accuracy > best_val_accuracy:
                best_val_accuracy = val_accuracy
                model_save_path = f"{saved_model_location}/TopKMoE"+"_best_model.pth"
                torch.save(model.state_dict(), model_save_path)
                print(f"New best model saved with accuracy: {best_val_accuracy:.2f}% at {model_save_path}")

# Example usage
train_and_validate(moe_model, train_loader, val_loader)