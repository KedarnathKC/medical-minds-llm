{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c8c36a2-4f38-40d4-9256-a851b1643ff1",
   "metadata": {
    "id": "7c8c36a2-4f38-40d4-9256-a851b1643ff1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import random\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c648608b-863d-4ae7-bc9c-4d87eb2c233a",
   "metadata": {
    "id": "c648608b-863d-4ae7-bc9c-4d87eb2c233a"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "expert1_count = torch.zeros(32, dtype = int)\n",
    "expert2_count = torch.zeros(32, dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4af2331e-32a5-4638-8c9d-36b73fefebdb",
   "metadata": {
    "id": "4af2331e-32a5-4638-8c9d-36b73fefebdb"
   },
   "outputs": [],
   "source": [
    "class GatingMechanism(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "        super(GatingMechanism, self).__init__()\n",
    "        self.gate = nn.Linear(input_dim, num_experts).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_mean = x.mean(dim=1)\n",
    "        gate_scores = F.softmax(self.gate(x_mean), dim=-1)  # Shape: [batch_size, num_experts]\n",
    "        return gate_scores.argmax(dim=-1)  # Shape: [batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfb9cfc6-f245-4963-82cc-7c45d82259f3",
   "metadata": {
    "id": "cfb9cfc6-f245-4963-82cc-7c45d82259f3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MoEModelWithPooling(nn.Module):\n",
    "    def __init__(self, experts, input_dim):\n",
    "        super().__init__()\n",
    "        self.experts = nn.ModuleList(experts)\n",
    "        self.num_layers = len(experts[0].base_model.model.model.layers)  # Correct path to access layers\n",
    "        self.gating = nn.ModuleList([GatingMechanism(input_dim, len(experts)) for _ in range(self.num_layers)])\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1).to(device)  # Example pooling layer\n",
    "        self.output_layer = nn.Linear(4096, 4).to(device)\n",
    "        # self.softmax = nn.Softmax(dim=1).to(device)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, training = True):\n",
    "        global expert2_count\n",
    "        global expert1_count\n",
    "\n",
    "        x = self.experts[0].base_model.model.model.embed_tokens(input_ids)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            expert_indices = self.gating[i](x)\n",
    "            \n",
    "            if training == False:\n",
    "                expert2_count[i] += int(expert_indices.sum())\n",
    "                expert1_count[i] += int(expert_indices.shape[0] - expert_indices.sum())\n",
    "                # print(expert1_count, expert2_count)\n",
    "\n",
    "            layer_output = torch.zeros_like(x)\n",
    "\n",
    "            for idx, expert in enumerate(self.experts):\n",
    "                mask = (expert_indices == idx).unsqueeze(-1).unsqueeze(1).half()\n",
    "\n",
    "                expert_input = x * mask\n",
    "\n",
    "                expert_output = expert.base_model.model.model.layers[i](expert_input, attn_mask=attention_mask)[0]\n",
    "                layer_output += expert_output * mask\n",
    "\n",
    "            x = layer_output\n",
    "\n",
    "        x = x.transpose(1, 2)  # Adjust dimensions for pooling\n",
    "        x = self.pooling(x).squeeze(2)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# GatingMechanism definition assumed to be implemented elsewhere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6F9nhynt5wMV",
   "metadata": {
    "id": "6F9nhynt5wMV"
   },
   "outputs": [],
   "source": [
    "# # %%capture\n",
    "# # Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00714f50-65fa-495f-b8de-04ee52f057ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00714f50-65fa-495f-b8de-04ee52f057ea",
    "outputId": "b359969d-ae5d-4d0a-febb-f7d500ff37e5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a44b62b236e4d7f8d5ea3bc52e231d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.5\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.352 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e98d26bdb944a6f9a00d4a29af38d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a2c3f5fb914044816e4411267bc804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13fbc69e2e0a44cfac1514267caa16ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.5\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.352 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = False.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90246c0bb6e48009a6a52f8431e81d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecde3d31413d4fc58dab1ede74bf39a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load pre-trained models\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 256 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model1, tokenizer = FastLanguageModel.from_pretrained(\"unsloth_domain2\",\n",
    "                                                     max_seq_length=max_seq_length,\n",
    "                                                     dtype=dtype,\n",
    "                                                     load_in_4bit=load_in_4bit)\n",
    "\n",
    "model2, tokenizer = FastLanguageModel.from_pretrained(\"ai2_arc_instruction_tuned_mistral_7b\",\n",
    "                                                     max_seq_length=max_seq_length,\n",
    "                                                     dtype=dtype,\n",
    "                                                     load_in_4bit=load_in_4bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32f26286-ead9-4963-a9a3-bf585643beb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in model1.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# for param in model2.parameters():\n",
    "#     param.requires_grad = False\n",
    "    \n",
    "models = [model1, model2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46dc4a0a-32b6-4726-9deb-ea44858d4216",
   "metadata": {
    "id": "46dc4a0a-32b6-4726-9deb-ea44858d4216"
   },
   "outputs": [],
   "source": [
    "num_layers = len(model1.base_model.model.model.layers)\n",
    "\n",
    "moe_model = MoEModelWithPooling(models, input_dim=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bfdc93f-e04b-4be6-95f1-bb58820ba7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable params  84164676\n"
     ]
    }
   ],
   "source": [
    "# print(\"Total params \", sum(p.numel() for p in model1.parameters()))\n",
    "print(\"Total trainable params \", sum(p.numel() for p in moe_model.parameters() if p.requires_grad))      \n",
    "\n",
    "# # a = 0\n",
    "# # for param in model1.parameters():\n",
    "# #     if param.requires_grad == True:\n",
    "# #         a += sum(param.numel())\n",
    "# # print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b6f328e-f7fb-4db9-97b9-90b36466e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in moe_model.named_parameters():\n",
    "#     if param.requires_grad == True:\n",
    "#         print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0ae064d-5ab4-46a6-91cf-8ca44cb421e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0ae064d-5ab4-46a6-91cf-8ca44cb421e7",
    "outputId": "2b520e41-76ea-4cc9-fd1a-df8ea03d9552"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoEModelWithPooling(\n",
      "  (experts): ModuleList(\n",
      "    (0-1): 2 x PeftModelForCausalLM(\n",
      "      (base_model): LoraModel(\n",
      "        (model): MistralForCausalLM(\n",
      "          (model): MistralModel(\n",
      "            (embed_tokens): Embedding(32000, 4096)\n",
      "            (layers): ModuleList(\n",
      "              (0-31): 32 x MistralDecoderLayer(\n",
      "                (self_attn): MistralSdpaAttention(\n",
      "                  (q_proj): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (k_proj): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (v_proj): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o_proj): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (rotary_emb): LlamaRotaryEmbedding()\n",
      "                )\n",
      "                (mlp): MistralMLP(\n",
      "                  (gate_proj): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=14336, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (up_proj): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=14336, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (down_proj): lora.Linear4bit(\n",
      "                    (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=14336, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (act_fn): SiLU()\n",
      "                )\n",
      "                (input_layernorm): MistralRMSNorm()\n",
      "                (post_attention_layernorm): MistralRMSNorm()\n",
      "              )\n",
      "            )\n",
      "            (norm): MistralRMSNorm()\n",
      "          )\n",
      "          (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (gating): ModuleList(\n",
      "    (0-31): 32 x GatingMechanism(\n",
      "      (gate): Linear(in_features=4096, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (pooling): AdaptiveAvgPool1d(output_size=1)\n",
      "  (output_layer): Linear(in_features=4096, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(moe_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bf72f79-3fba-4310-85fe-99f777efd4b0",
   "metadata": {
    "id": "8bf72f79-3fba-4310-85fe-99f777efd4b0"
   },
   "outputs": [],
   "source": [
    "# from torch.cuda.amp import GradScaler\n",
    "\n",
    "# scaler = GradScaler()\n",
    "\n",
    "# temp = torch.ones((2,8), dtype=torch.int64).to(device)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# labels = torch.rand(2, 4).float().to(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(moe_model.parameters(), lr=1e-3)\n",
    "\n",
    "# optimizer.zero_grad()\n",
    "# with torch.cuda.amp.autocast():\n",
    "#     output = moe_model(temp).float()\n",
    "#     loss = criterion(output, labels.float())\n",
    "\n",
    "# scaler.scale(loss).backward()\n",
    "# scaler.step(optimizer)\n",
    "# scaler.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "HdvpSKpvy2d_",
   "metadata": {
    "id": "HdvpSKpvy2d_"
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "Pe8PHYbH2ncc",
   "metadata": {
    "id": "Pe8PHYbH2ncc"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk, concatenate_datasets, Dataset\n",
    "\n",
    "dataset_location = 'medmcqa-prompts'\n",
    "\n",
    "train_dataset = load_from_disk(f\"{dataset_location}/train_prompts.hf\")\n",
    "# test_dataset = load_from_disk(f\"{dataset_location}/test_prompts.hf\")\n",
    "eval_dataset = load_from_disk(f\"{dataset_location}/eval_prompts.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "Km-2uA852uY2",
   "metadata": {
    "id": "Km-2uA852uY2"
   },
   "outputs": [],
   "source": [
    "# train = []\n",
    "# val = []\n",
    "# count = 0\n",
    "# for i in train_dataset:\n",
    "#     train.append(i)\n",
    "#     count += 1\n",
    "#     if count >= 100:\n",
    "#         break\n",
    "\n",
    "# count = 0\n",
    "# for i in eval_dataset:\n",
    "#     val.append(i)\n",
    "#     count += 1\n",
    "#     if count >= 100:\n",
    "#         break\n",
    "\n",
    "# train_dataset = ''\n",
    "# eval_dataset = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "HQuiO-Kb2wlO",
   "metadata": {
    "id": "HQuiO-Kb2wlO"
   },
   "outputs": [],
   "source": [
    "# print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "MIfB2l0j2x7c",
   "metadata": {
    "id": "MIfB2l0j2x7c"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class MCQDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)  # Changed to float for one-hot encoding\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Function to encode the data\n",
    "def encode_data(tokenizer, prompts):\n",
    "    encodings = tokenizer(prompts, truncation=True, padding=True, max_length = 128)\n",
    "    return encodings\n",
    "\n",
    "# Prepare the data for tokenization\n",
    "prompts = [item['prompt'] for item in train_dataset]\n",
    "labels = [item['label_one_hot'] for item in train_dataset]  # one-hot encoded labels\n",
    "\n",
    "# Tokenize data\n",
    "encodings = encode_data(tokenizer, prompts)\n",
    "\n",
    "# Create dataset\n",
    "train_set = MCQDataset(encodings, labels)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "prompts = [item['prompt'] for item in eval_dataset]\n",
    "labels = [item['label_one_hot'] for item in eval_dataset]  # one-hot encoded labels\n",
    "\n",
    "# Tokenize data\n",
    "encodings = encode_data(tokenizer, prompts)\n",
    "\n",
    "# Create dataset\n",
    "eval_set = MCQDataset(encodings, labels)\n",
    "\n",
    "# DataLoader\n",
    "val_loader = DataLoader(eval_set, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "zj9aBgOW2yL_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zj9aBgOW2yL_",
    "outputId": "7ceb5a87-ceb9-4f15-ae6c-c4c75aff68d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "Input IDs: torch.Size([16, 128])\n",
      "Attention Mask: torch.Size([16, 128])\n",
      "Labels: torch.Size([16, 4])\n",
      "First input IDs example: tensor([    1, 28705,    13,  2287, 22478, 28747,    13,  2287,   330, 28705,\n",
      "        28740, 28782, 28733,  4395, 28733,   738,  4531,  7567,   395, 28705,\n",
      "        28740,  1370,  3340,   302, 25352,   319,  7610, 28725,  1083,   514,\n",
      "        28768, 18181,  3098,  8012,   286,   304,  3276, 14692,   294,   408,\n",
      "         1029, 28723, 11606,   326,   697, 10924, 28747,   382, 28726, 28705,\n",
      "        28784, 28723, 28781,   319, 28719, 28748, 28715, 28758, 28745,   320,\n",
      "         9162, 28733, 28750, 28784, 28725, 28782, 28734, 28734, 28748,  3221,\n",
      "        28770, 28745,   430,   362,   436,  5721,   727, 28733, 28750, 28734,\n",
      "         5267,   395,   264,  2602,   302, 28705, 28740, 28770,  5267, 28745,\n",
      "        10473,   306,   436, 28726,   410,  4081,   262,   727, 28733, 28782,\n",
      "        28734,  5267, 28745,   304,   285,  2792,   262,  8371, 28705, 28740,\n",
      "        28734, 18144, 28748, 28715, 28758, 28723,  2744, 10447,   282,   991,\n",
      "          644,   403,  3397,   495,   302,  1183,  1723,   586])\n",
      "First attention mask example: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "First label example: tensor([0., 0., 1., 0.])\n",
      "Batch 1\n",
      "Input IDs: torch.Size([16, 128])\n",
      "Attention Mask: torch.Size([16, 128])\n",
      "Labels: torch.Size([16, 4])\n",
      "Batch 2\n",
      "Input IDs: torch.Size([16, 128])\n",
      "Attention Mask: torch.Size([16, 128])\n",
      "Labels: torch.Size([16, 4])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    print(\"Batch\", i)\n",
    "    print(\"Input IDs:\", batch['input_ids'].shape)\n",
    "    print(\"Attention Mask:\", batch['attention_mask'].shape)\n",
    "    print(\"Labels:\", batch['labels'].shape)\n",
    "\n",
    "    # Print the actual content of the first example in the batch\n",
    "    if i == 0:\n",
    "        print(\"First input IDs example:\", batch['input_ids'][0])\n",
    "        print(\"First attention mask example:\", batch['attention_mask'][0])\n",
    "        print(\"First label example:\", batch['labels'][0])\n",
    "\n",
    "    # Optionally, break after a few batches to avoid too much output\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wz2uVnHJ2zop",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "Wz2uVnHJ2zop",
    "outputId": "336d2cdb-cb82-415a-b1d5-a4f13b138b33"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [TRAIN]:   0%|          | 1/11427 [00:03<9:35:59,  3.02s/batch, loss=1.42, temp_acc=12.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.4234427213668823\n",
      "Temp accuracy:  12.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [TRAIN]:   9%|▉         | 1001/11427 [34:54<6:02:08,  2.08s/batch, loss=1.38, temp_acc=26.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1.3806685209274292\n",
      "Temp accuracy:  26.523476523476525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [TRAIN]:  18%|█▊        | 2001/11427 [1:09:41<5:27:05,  2.08s/batch, loss=1.55, temp_acc=27.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 1.546600103378296\n",
      "Temp accuracy:  27.09895052473763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [TRAIN]:  26%|██▋       | 3001/11427 [1:44:26<4:51:34,  2.08s/batch, loss=3.66, temp_acc=27]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 3.6552422046661377\n",
      "Temp accuracy:  26.99933355548151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [TRAIN]:  35%|███▌      | 4001/11427 [2:19:03<4:16:44,  2.07s/batch, loss=1.78, temp_acc=26.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 1.7825253009796143\n",
      "Temp accuracy:  26.69645088727818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [TRAIN]:  39%|███▉      | 4459/11427 [2:34:55<4:01:01,  2.08s/batch, loss=1.98, temp_acc=26.6]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "scaler = GradScaler()\n",
    "\n",
    "def print_memory_usage():\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "def compute_loss(outputs, labels, attention_mask):\n",
    "    # Flatten the outputs and labels for loss calculation\n",
    "    active_loss = attention_mask.view(-1) == 1  # Mask out padded tokens\n",
    "    active_logits = outputs.view(-1, outputs.size(-1))[active_loss]\n",
    "    active_labels = labels.view(-1)[active_loss]\n",
    "    return F.cross_entropy(active_logits, active_labels)\n",
    "\n",
    "def train_and_validate(model, train_loader, val_loader, log_file_path, epochs=15):\n",
    "    scaler = GradScaler()\n",
    "    device = torch.device(\"cuda\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-4, eps=1e-8)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_accuracy = 0\n",
    "    \n",
    "    with open(log_file_path, 'a') as log_file:\n",
    "        log_file.write(\"Starting training process...\\n\")\n",
    "        log_file.flush()\n",
    "        for epoch in range(epochs):\n",
    "            total_train_loss = 0\n",
    "            total_train_correct = 0\n",
    "            train_samples = 0\n",
    "\n",
    "            model.train()\n",
    "            train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1} [TRAIN]\", unit=\"batch\")\n",
    "            for i, batch in enumerate(train_pbar):\n",
    "                input_ids, labels, attention_mask = batch['input_ids'].to(device), batch['labels'].to(device), batch['attention_mask'].to(dtype=bool, device=device)\n",
    "                train_samples += labels.size(0)\n",
    "#                 mask = attention_mask.unsqueeze(1)\n",
    "\n",
    "#                 mask = mask.expand(-1, 128, -1)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    # print(attention_mask.shape)\n",
    "                    output = model(input_ids, attention_mask, True).float()\n",
    "                    loss = criterion(output, labels.float())\n",
    "                    predictions = torch.argmax(F.softmax(output,dim=1), dim=1)\n",
    "                    labels_indices = torch.argmax(labels, dim=1)\n",
    "                    total_train_correct += (predictions == labels_indices).sum().item()\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                total_train_loss += loss.item()\n",
    "                \n",
    "                log_file.write(f\"Batch {i}, Epoch {epoch+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {100 * total_train_correct / train_samples:.2f}%\\n\")\n",
    "                log_file.flush()\n",
    "                \n",
    "                train_pbar.set_postfix(loss=loss.item(), temp_acc=100 * total_train_correct / train_samples)\n",
    "\n",
    "                if i % 1000 == 0:\n",
    "                    print(i, loss.item())\n",
    "                    print(f\"Temp accuracy: \", total_train_correct / train_samples * 100)\n",
    "\n",
    "            avg_train_loss = total_train_loss / len(train_loader)\n",
    "            train_accuracy = total_train_correct / train_samples * 100\n",
    "            print(f\"Training Accuracy: \", train_accuracy)\n",
    "            print(f\"Epoch {epoch+1}, Loss: {avg_train_loss}\")\n",
    "\n",
    "            model.eval()\n",
    "            total_val_loss, val_samples, total_val_correct = 0, 0, 0\n",
    "            with torch.no_grad():\n",
    "                for i, batch in enumerate(val_loader):\n",
    "                    input_ids, labels, attention_mask = batch['input_ids'].to(device), batch['labels'].to(device), batch['attention_mask'].to(device)\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        output = model(input_ids, attention_mask, False).float()\n",
    "                        val_loss = criterion(output, labels.float())\n",
    "                        predictions = torch.argmax(F.softmax(output,dim=1), dim=1)\n",
    "                        labels_indices = torch.argmax(labels, dim=1)\n",
    "                        total_val_correct += (predictions == labels_indices).sum().item()\n",
    "\n",
    "                    total_val_loss += val_loss.item()\n",
    "                    val_samples += labels.size(0)\n",
    "                    log_file.write(f\"Batch {i}, Epoch {epoch+1}, Validation Loss: {loss.item():.4f}, Validation Accuracy: {100 * total_val_correct / val_samples:.2f}% Expert 1 - {expert1_count}, Expert 2 - {expert2_count}\\n\")\n",
    "                    log_file.flush()\n",
    "                    \n",
    "            avg_val_loss = total_val_loss / len(val_loader)\n",
    "            val_accuracy = total_val_correct / val_samples * 100\n",
    "            print(f\"Validation Accuracy: \", val_accuracy)\n",
    "            print(f\"Epoch {epoch+1} - Validation Loss: {avg_val_loss:.4f}\") \n",
    "            \n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                best_model_path = \"best_model_full2.pth\"\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                print(f\"New best model saved with accuracy: {best_val_accuracy:.2f}% at {best_model_path}\")\n",
    "\n",
    "# Example usage\n",
    "train_and_validate(moe_model, train_loader, val_loader, 'training_final_mega2.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ebbbc9-1805-46ce-8e42-a7127e050b51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
